{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Quantum Neuron Born Machine\n",
        "**Quantum Neuron Born Machine** (QNBM) is a model that adds non-linear activations via a neural network structure onto the standard Born Machine framework. The QNBM utilizes a previously introduced Quantum Neuron subroutine, which is a repeat-until-success circuit with mid-circuit measurements and classical control. The QNBM has been shown to achieve an almost 3x smaller error rate than a linear Quantum Circuit Born Machine (QCBM) with a similar number of tunable parameters. This suggests that non-linearity is a useful resource in quantum generative models, and the QNBM is a new model with good generative performance and potential for quantum advantage.\n",
        "\n",
        "\n",
        "## Connect to the Azure Quantum workspace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preparing Q# environment...\n"
          ]
        },
        {
          "data": {
            "application/x-qsharp-data": "\"Connecting to Azure Quantum...\"",
            "text/plain": [
              "Connecting to Azure Quantum..."
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Authenticated using Microsoft.Azure.Quantum.Authentication.TokenFileCredential\n",
            "\n",
            "\n",
            "Connected to Azure Quantum workspace QuantumDemo in location westcentralus.\n",
            "This workspace's targets:\n",
            "- ionq.qpu\n",
            "- ionq.qpu.aria-1\n",
            "- ionq.simulator\n",
            "- microsoft.estimator\n",
            "- quantinuum.qpu.h1-1\n",
            "- quantinuum.sim.h1-1sc\n",
            "- quantinuum.qpu.h1-2\n",
            "- quantinuum.sim.h1-2sc\n",
            "- quantinuum.sim.h1-1e\n",
            "- quantinuum.sim.h1-2e\n",
            "- rigetti.sim.qvm\n",
            "- rigetti.qpu.aspen-m-2\n",
            "- rigetti.qpu.aspen-m-3\n"
          ]
        }
      ],
      "source": [
        "import qsharp\n",
        "import qsharp.azure\n",
        "\n",
        "targets = qsharp.azure.connect(\n",
        "            resourceId = \"\",\n",
        "            location = \"\")\n",
        "\n",
        "\n",
        "print(\"This workspace's targets:\")\n",
        "for target in targets:\n",
        "    print(\"-\", target.id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "import typing\n",
        "import math\n",
        "\n",
        "\n",
        "def counts_to_prob(self, samples: typing.Dict): #Turns a dictionary of sample counts into probabilities.\n",
        "    total_shots = np.sum(list(samples.values()))\n",
        "    for bitstring, count in samples.items(): \n",
        "        samples[bitstring] = count / total_shots \n",
        "    return samples \n",
        "\n",
        "def dec2bin(number: int, length: int) -> typing.List[int]: #Turns a decimal number into a binary bitstring.\n",
        "    bit_str = bin(number)\n",
        "    bit_str = bit_str[2 : len(bit_str)]  \n",
        "    bit_string = [int(x) for x in list(bit_str)]\n",
        "    if len(bit_string) < length:\n",
        "        len_zeros = length - len(bit_string)\n",
        "        bit_string = [int(x) for x in list(np.zeros(len_zeros))] + bit_string\n",
        "    return bit_string\n",
        "\n",
        "def comb(n: int, k: int)-> int: #Combinatorial method for n choose k.\n",
        "    return math.factorial(n) // math.factorial(k) // math.factorial(n - k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from qiskit import Aer,QuantumCircuit, QuantumRegister, ClassicalRegister, execute\n",
        "from qiskit.providers.ibmq.runtime import UserMessenger, ProgramBackend\n",
        "from qiskit.algorithms.optimizers import NELDER_MEAD, ADAM, GradientDescent\n",
        "from qiskit.providers.aer import AerSimulator\n",
        "from qiskit import transpile\n",
        "import numpy as np\n",
        "from numpy import pi\n",
        "import math\n",
        "\n",
        "class QuantumNeuralNetwork:\n",
        "\n",
        "    def __init__(self, width_list, k):\n",
        "        self.structure = width_list\n",
        "        self.depth = len(width_list)\n",
        "        self.edge_no = self.EdgeNo() \n",
        "        self.neuron_no = sum(width_list) \n",
        "        self.ancilla_qubit_no = k \n",
        "        self.cbits_per_neuron = 2**k - 1\n",
        "        self.ancilla_bit_no = (self.neuron_no - self.structure[0]) * (2**k - 1)\n",
        "        self.answer_bit_no = self.structure[self.depth - 1]\n",
        "        self.qubit_no = self.neuron_no + self.ancilla_qubit_no \n",
        "        self.bit_no = self.ancilla_bit_no + self.answer_bit_no\n",
        "\n",
        "    def EdgeNo(self): #Defines the number of edges in the model \n",
        "        edgeno = 0\n",
        "        for i in range(len(self.structure)-1):\n",
        "            edgeno += self.structure[i] * self.structure[i+1]\n",
        "        return edgeno\n",
        "\n",
        "    def Normalise(self, weights_unnormalised, biases_unnormalised): #Normalizes the weights and biases of the model \n",
        "        weight_counter = 0\n",
        "        bias_counter = 0\n",
        "        weights = np.copy(weights_unnormalised)\n",
        "        biases = np.copy(biases_unnormalised)\n",
        "        for i in range(self.depth-1):\n",
        "            current_layer = self.structure[i]\n",
        "            next_layer = self.structure[i+1]\n",
        "            normalisation = pi/4 / (current_layer + 1)\n",
        "            no_of_weights = current_layer * next_layer\n",
        "            no_of_biases = next_layer\n",
        "            weights[weight_counter : weight_counter + no_of_weights] = weights[weight_counter : weight_counter + no_of_weights] * normalisation\n",
        "            biases[bias_counter : bias_counter + no_of_biases] = biases[bias_counter : bias_counter + no_of_biases] * normalisation + pi/4\n",
        "            weight_counter += no_of_weights\n",
        "            bias_counter += no_of_biases\n",
        "        return weights, biases\n",
        "        \n",
        "    def CreateCircuit(self, weights_unnorm, biases_unnorm, initialisation = 'h'): #Defines the neruon circuit using the weights and biases \n",
        "\n",
        "        circ = QuantumCircuit(self.qubit_no, self.bit_no)\n",
        "        first_layer_size = self.structure[0]\n",
        "        weights, biases = self.Normalise(weights_unnorm, biases_unnorm)\n",
        "        print(\"Normalized weights and biases: \", weights, biases)\n",
        "        if initialisation == 'h':\n",
        "            for i in range(self.ancilla_qubit_no, self.ancilla_qubit_no + first_layer_size):\n",
        "                circ.h(i)\n",
        "        else:\n",
        "            circ = circ.compose(initialisation, range(self.ancilla_qubit_no, self.ancilla_qubit_no + first_layer_size))\n",
        "        qubit_counter = self.ancilla_qubit_no \n",
        "        weight_counter = 0\n",
        "        bias_counter = 0 \n",
        "        cbit_counter = 0 \n",
        "        for i in range(self.depth-1):\n",
        "            circ, qubit_counter, weight_counter, bias_counter, cbit_counter = self.AddLayer(circ, qubit_counter, weight_counter, bias_counter, cbit_counter, i, weights, biases)\n",
        "        circ.measure(range(self.qubit_no - self.answer_bit_no, self.qubit_no), range(self.ancilla_bit_no, self.bit_no))\n",
        "        return circ\n",
        "\n",
        "    def AddLayer(self, circ, qubit_counter, weight_counter, bias_counter, cbit_counter, i, weights, biases): #Adds a neuron layer to the circuit  \n",
        "        previous_layer_size = self.structure[i]\n",
        "        next_layer_size = self.structure[i+1]\n",
        "        previous_layer = range(qubit_counter, qubit_counter + previous_layer_size)\n",
        "        next_layer = range(qubit_counter + previous_layer_size, qubit_counter + previous_layer_size + next_layer_size) \n",
        "\n",
        "        for output_qubit in next_layer:\n",
        "            relevant_weights = weights[weight_counter : weight_counter + previous_layer_size]\n",
        "            relevant_bias = biases[bias_counter]\n",
        "            circ = self.ConnectLayerToNode(circ, cbit_counter, previous_layer, output_qubit, relevant_weights, relevant_bias, self.ancilla_qubit_no)\n",
        "            cbit_counter += self.cbits_per_neuron\n",
        "            weight_counter += previous_layer_size\n",
        "            bias_counter += 1\n",
        "        qubit_counter += previous_layer_size\n",
        "        return circ, qubit_counter, weight_counter, bias_counter, cbit_counter\n",
        "\n",
        "    def ConnectLayerToNode(self, circ, cbit_counter, previous_layer, output_qubit, weights, bias, k): #Connects the previous layer to a neuron in the next layer  \n",
        "        if k == 1:\n",
        "            for i, input_qubit_i in enumerate(previous_layer):\n",
        "                weight_i = weights[i]\n",
        "                circ.cry(2*weight_i, input_qubit_i, 0)\n",
        "            circ.ry(2*bias, 0)\n",
        "            circ.cy(0, output_qubit)\n",
        "            circ.rz(-pi/2, 0)\n",
        "            for i, input_qubit_i in enumerate(previous_layer):\n",
        "                weight_i = weights[i]\n",
        "                circ.cry(-2*weight_i, input_qubit_i, 0)\n",
        "            circ.ry(-2*bias, 0)\n",
        "            circ.measure(0, cbit_counter)\n",
        "            circ.reset(0)\n",
        "        else:\n",
        "            circ = self.ConnectLayerToNode(circ, cbit_counter, previous_layer, k-1, weights, bias, k-1)\n",
        "            circ.cy(k-1, output_qubit)\n",
        "            circ.rz(-pi/2, k-1)\n",
        "            circ = self.ConnectLayerToNode(circ, cbit_counter + 2**(k-1) - 1, previous_layer, k-1, -1*weights, -1*bias, k-1)\n",
        "            circ.measure(k-1, cbit_counter + 2**k - 2)\n",
        "            circ.reset(k-1)\n",
        "        return circ\n",
        "\n",
        "    def CreateCircuitList(self, weights_list, biases_list): #Keeps track of the weights and biases in the model \n",
        "        list = []\n",
        "        for i in range(len(weights_list)):\n",
        "            weights = weights_list[i]\n",
        "            biases = biases_list[i]\n",
        "            list.append(self.CreateCircuit(weights,biases))\n",
        "        return list\n",
        "\n",
        "    def FindProbs(self, counts, epsilon = 0, datatype = 'dict'): #Converts the circuit counts to an approximate probability distribution \n",
        "        new_counts = {}\n",
        "        probs_dict = {}\n",
        "        sum = 0\n",
        "        for count in counts:\n",
        "            if count[self.bit_no - self.ancilla_bit_no:] == '0'*self.ancilla_bit_no:\n",
        "                new_counts[count[0:self.answer_bit_no]] = counts[count]\n",
        "                sum += counts[count]\n",
        "        for new_count in new_counts:\n",
        "            probs_dict[new_count] = new_counts[new_count]/sum\n",
        "        if datatype == 'dict':\n",
        "            return probs_dict\n",
        "        else:\n",
        "            if datatype == 'array':\n",
        "                return self.ProbsDict_to_ProbsArray(probs_dict, epsilon)\n",
        "            else:\n",
        "                print('only dict and array are accepted as types')\n",
        "                return\n",
        "\n",
        "    def ProbsDict_to_ProbsArray(self, probs_dict,epsilon): #Converts the distribution dictionary to an array \n",
        "        keys = list(probs_dict.keys())\n",
        "        keyno = len(keys)\n",
        "        bitno = len(keys[0])\n",
        "        nocounts = 2**bitno - keyno\n",
        "        beta = epsilon * nocounts / keyno\n",
        "        probs_array = np.zeros(2**bitno)\n",
        "        for i in range(2**bitno):\n",
        "            i_bin = format(i, '0'+str(bitno)+'b')\n",
        "            if i_bin in probs_dict:\n",
        "                probs_array[i] = probs_dict[i_bin] - beta\n",
        "            else:\n",
        "                probs_array[i] = epsilon\n",
        "        return probs_array\n",
        "\n",
        "    def FindCounts(self, counts): #Gets the correct counts of the output neurons \n",
        "        new_counts = {}\n",
        "        for count in counts:\n",
        "            if count[self.bit_no - self.ancilla_bit_no:] == '0'*self.ancilla_bit_no:\n",
        "                new_counts[count[0:self.answer_bit_no]] = counts[count]\n",
        "        return new_counts\n",
        "    def KL(self, trained_distribution, target_distribution,epsilon):\n",
        "        target_distribution = self.ProbsDict_to_ProbsArray(target_distribution, epsilon)\n",
        "        trained_distribution = self.ProbsDict_to_ProbsArray(trained_distribution, epsilon)\n",
        "        return np.dot(target_distribution, np.log(target_distribution/trained_distribution))\n",
        "        \n",
        "def main(backend: ProgramBackend, user_messenger: UserMessenger, target_distribution, structure, k, initial_weights, initial_biases, maxiter, eps, lr, seed, gradient = \"finite\"): #Runs the QNBM algorithm \n",
        "    epsilon = 1e-16 \n",
        "    weight_no = len(initial_weights)\n",
        "    bias_no = len(initial_biases)\n",
        "    initial_params = np.concatenate([initial_weights, initial_biases])\n",
        "    qnn = QuantumNeuralNetwork(structure, k)\n",
        "\n",
        "    iteration_list = []\n",
        "    params_list = []\n",
        "    KL_list = []\n",
        "    \n",
        "    def runqnn(parameters): \n",
        "        shots = 102400\n",
        "        params_list.append(parameters)\n",
        "        w = parameters[0:weight_no]\n",
        "        b = parameters[weight_no:]\n",
        "        circuit = qnn.CreateCircuit(w,b)\n",
        "        tcirc = transpile(circuit, backend)\n",
        "\n",
        "        # Execute noisy simulation and get counts\n",
        "        result = backend.run(tcirc, shots=shots).result()\n",
        "        counts = result.get_counts()\n",
        "        \n",
        "        probs = qnn.FindProbs(counts, epsilon, 'array')\n",
        "        KL_list.append(KL(probs))\n",
        "        iteration_list.append(1)\n",
        "        if len(iteration_list) % len(parameters) == 0: \n",
        "            print(KL(probs))\n",
        "        return KL(probs)    \n",
        "       \n",
        "    def KL(trained_distribution):\n",
        "        return np.dot(target_distribution, np.log(target_distribution/trained_distribution))\n",
        "    \n",
        "    var_bounds = [(-1,1)]*(weight_no+bias_no)\n",
        "    np.random.seed(seed)\n",
        "    optimizer = ADAM(maxiter = maxiter, eps=eps, lr = lr) \n",
        "    if gradient == \"finite\":\n",
        "        result = optimizer.minimize(runqnn,\n",
        "                       initial_params,\n",
        "                       bounds = var_bounds)\n",
        " \n",
        "    params_list.append(result.x)\n",
        "    KL_list.append(result.fun)\n",
        "    return params_list, KL_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "import typing\n",
        "\n",
        "def subset_distribution( #Generates a cardinality/hamming training probability distribution in the form of a dictionary\n",
        "    n_qubits: int, \n",
        "    numerality: int\n",
        "    ) -> typing.Dict: \n",
        "    dict = {}\n",
        "    prob = 1 / comb(n_qubits, numerality)\n",
        "    for i in range(2**n_qubits):\n",
        "        i_bin = format(i, '0'+str(n_qubits)+'b')\n",
        "        counter = 0\n",
        "        for bit in i_bin:\n",
        "            if bit == '1':\n",
        "                counter += 1\n",
        "        if counter == numerality:\n",
        "            dict[i_bin] = prob\n",
        "    return dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "microsoft": {
          "language": "qsharp"
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%%qsharp\n",
        "open Microsoft.Quantum.Intrinsic;     // for H\n",
        "open Microsoft.Quantum.Measurement;   // for MResetZ\n",
        "open Microsoft.Quantum.Canon;         // for ApplyToEach\n",
        "open Microsoft.Quantum.Arrays;        // for ForEach\n",
        "open Microsoft.Quantum.Core;\n",
        "open Microsoft.Quantum.Math;\n",
        "open Microsoft.Quantum.Convert;\n",
        "\n",
        "@EntryPoint()\n",
        "operation createCircuitQ(): (Result[]){\n",
        "    //1, 0, 2\n",
        "    let weights = [ 1.34323741, -1.32426818];\n",
        "    let biases = [0.07816141, 1.41207129];\n",
        "\n",
        "    let structure = [1, 2];\n",
        "    let depth = 2;\n",
        "    let ancilla_qubit_no = 1;\n",
        "    let neuron_no = 3; //ask about this\n",
        "    let first_layer_size = structure[0];//1\n",
        "    let answer_bit_no = structure[depth - 1]; //1\n",
        "    let qubit_no = neuron_no + ancilla_qubit_no; //3\n",
        "    \n",
        "    let num_output = structure[depth - 1]; //1\n",
        "\n",
        "    use qs = Qubit[qubit_no];\n",
        "    let end = (ancilla_qubit_no + first_layer_size) - 1; //1\n",
        "    for qubit in ancilla_qubit_no .. end {\n",
        "        H(qs[qubit]);\n",
        "    }\n",
        "    //All mutables that are needed\n",
        "    mutable qubit_counter = ancilla_qubit_no; //1\n",
        "    mutable weight_counter = 0;\n",
        "    mutable bias_counter = 0;\n",
        "\n",
        "    for i in 0 .. depth - 2 {\n",
        "            set (qubit_counter, weight_counter, bias_counter)\n",
        "            = addLayer(qs, structure, qubit_counter, weight_counter, bias_counter, i, weights, biases, ancilla_qubit_no);\n",
        "    }\n",
        "    return MeasureEachZ(qs[qubit_no - answer_bit_no .. (qubit_no-1)]); //2 .. 2\n",
        "}\n",
        "\n",
        "operation addLayer(qs: Qubit[], structure: Int[], qubit_counter: Int, weight_counter: Int, bias_counter: Int, i: Int, weights: Double[], biases: Double[], ancilla_qubit_no: Int): (Int, Int, Int){\n",
        "        mutable weight_counter_new = weight_counter; //0\n",
        "        mutable bias_counter_new = bias_counter; //0\n",
        "        mutable qubit_counter_new = qubit_counter; //1\n",
        "        let previous_layer_size = structure[i]; //1\n",
        "        let next_layer_size = structure[i+1]; //1\n",
        "        mutable counter = 0;\n",
        "        let previous_layer = qubit_counter..(qubit_counter+previous_layer_size-1); //1...1\n",
        "        let next_layer = qubit_counter+previous_layer_size..(qubit_counter + previous_layer_size + next_layer_size - 1); //2...2\n",
        "\n",
        "        for output_qubit in next_layer{\n",
        "                //int that will store the results of the mid-circuit measurements; forcing type to be Int instead of Result -> initialize to 2\n",
        "            Message($\"output_qubit layer: {output_qubit} \");\n",
        "            mutable succeeded = false;\n",
        "            for count in 1 .. 7{ //RUS Circuits iterations (for success) are expected to be less than 7\n",
        "                if not succeeded{\n",
        "                    let endPos = weight_counter_new + previous_layer_size - 1; //0\n",
        "                    let relevant_weights = weights[weight_counter_new .. endPos]; //weights[0]\n",
        "                    let relevant_bias = biases[bias_counter_new]; //biases[0]\n",
        "                    let cb = measuringAncilla(qs, previous_layer, output_qubit, relevant_weights, relevant_bias, ancilla_qubit_no);\n",
        "                    if cb == Zero{\n",
        "                        set succeeded = true;\n",
        "                    }\n",
        "                    else{ //if the mid-measurement for the previous RUS circuit came out to be 1 (RUS did not succeed)... \n",
        "                        //Ancilla qubit needs to be reset\n",
        "                        X(qs[0]);\n",
        "                        Ry(((PI()*-1.0)/2.0), qs[output_qubit-1]);\n",
        "                    }\n",
        "                    //Storing the mid circuit measurement\n",
        "                    // 2 output neurons\n",
        "                    // output neuron 1 \n",
        "                    //1 1 1 1...0 -> 110\n",
        "                }\n",
        "            }\n",
        "            set weight_counter_new = weight_counter_new + previous_layer_size; //1\n",
        "            set bias_counter_new = bias_counter_new + 1; //1\n",
        "            set counter = counter + 1; //1\n",
        "        }\n",
        "        set qubit_counter_new = qubit_counter_new + previous_layer_size; //2\n",
        "        return (qubit_counter_new, weight_counter_new, bias_counter_new);\n",
        "}\n",
        "\n",
        "operation measuringAncilla(qs: Qubit[], previous_layer: Range, output_qubit: Int, weights: Double[], bias: Double, k: Int): Result{\n",
        "        connectLayerToNode(qs, previous_layer, output_qubit, weights, bias, k);\n",
        "        if k == 1{\n",
        "            let result =  M(qs[0]);\n",
        "            Reset(qs[0]);\n",
        "            return result;\n",
        "        }\n",
        "        else{\n",
        "            let result = M(qs[k-1]);\n",
        "            Reset(qs[k-1]);\n",
        "            return result;\n",
        "        }\n",
        "    \n",
        "}\n",
        "\n",
        "function Negated(arr : Double[]) : Double[] {   \n",
        "        mutable negated = [0.0, size=Length(arr)];    \n",
        "        for idx in IndexRange(arr) {        \n",
        "            set negated w/= idx <- -arr[idx];    \n",
        "        }\n",
        "        return negated;\n",
        "}\n",
        "\n",
        "operation connectLayerToNode(qs: Qubit[], previous_layer: Range, output_qubit: Int, weights: Double[], bias: Double, k: Int): Unit is Ctl { \n",
        "        if k == 1{\n",
        "            for i in previous_layer{  //1..1\n",
        "                let weight_i = weights[i-1]; //weights[0]\n",
        "                Controlled Ry([qs[i]], ((weight_i*2.0), qs[0]));\n",
        "            }\n",
        "            Ry((bias*2.0), qs[0]);\n",
        "            Controlled Y([qs[0]], (qs[output_qubit]));\n",
        "            Rz(((PI()*-1.0)/2.0), qs[0]);\n",
        "            for i in previous_layer{\n",
        "                let weight_i = weights[i-1]; \n",
        "                Controlled Ry([qs[i]], ((weight_i*-2.0), qs[0]));\n",
        "            }\n",
        "            Ry((bias*-2.0), qs[0]);\n",
        "        }\n",
        "        else{\n",
        "            connectLayerToNode(qs, previous_layer, k-1, weights, bias, k-1);\n",
        "            Controlled Y([qs[k-1]], (qs[output_qubit]));\n",
        "            Rz(((PI()*-1.0)/2.0), qs[k-1]);  \n",
        "            connectLayerToNode(qs, previous_layer, k-1, Negated(weights), (-1.0*bias), k-1);\n",
        "        }\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading package Microsoft.Quantum.Providers.Honeywell and dependencies...\n",
            "Active target is now quantinuum.sim.h1-2e\n",
            "Submitting createCircuitQ to target quantinuum.sim.h1-2e...\n",
            "Job successfully submitted.\n",
            "   Job name: QNBM\n",
            "   Job ID: bcf6c2bc-1962-4931-b1c1-1597d43cd922\n",
            "Waiting up to 240 seconds for Azure Quantum job to complete...\n",
            "[18:23:34] Current job status: Waiting\n",
            "[18:23:39] Current job status: Waiting\n",
            "[18:23:44] Current job status: Waiting\n",
            "[18:23:49] Current job status: Executing\n",
            "[18:23:54] Current job status: Executing\n",
            "[18:23:59] Current job status: Executing\n",
            "[18:24:05] Current job status: Executing\n",
            "[18:24:10] Current job status: Executing\n",
            "[18:24:15] Current job status: Executing\n",
            "[18:24:20] Current job status: Executing\n",
            "[18:24:25] Current job status: Executing\n",
            "[18:24:30] Current job status: Executing\n",
            "[18:24:35] Current job status: Executing\n",
            "[18:24:40] Current job status: Executing\n",
            "[18:24:45] Current job status: Executing\n",
            "[18:24:50] Current job status: Succeeded\n"
          ]
        }
      ],
      "source": [
        "#qsharp.azure.target(\"quantinuum.sim.h1-1sc\")\n",
        "#qsharp.azure.target(\"quantinuum.sim.h1-1e\")\n",
        "qsharp.azure.target(\"quantinuum.sim.h1-2e\")\n",
        "\n",
        "qsharp.azure.target_capability(\"AdaptiveExecution\")\n",
        "\n",
        "# We'll use 10 shots (simulated runs). Timeout is in seconds.\n",
        "result = qsharp.azure.execute(createCircuitQ, shots=10, jobName=\"QNBM\", timeout=240)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/x-qsharp-data": "{\"[1, 0]\":0.7,\"[0, 1]\":0.3}",
            "text/html": [
              "\r\n",
              "                    <table style=\"table-layout: fixed; width: 100%\">\r\n",
              "                        <thead>\r\n",
              "                            <tr>\r\n",
              "                                <th style=\"text-align: left; width: 25ch\">Result</th>\r\n",
              "                                <th style=\"text-align: left; width: 25ch\">Frequency</th>\r\n",
              "                                <th style=\"text-align: left; width: calc(100% - 25ch - 25ch)\">Histogram</th>\r\n",
              "                            </tr>\r\n",
              "                        </thead>\r\n",
              "                        <tbody>\r\n",
              "                            \r\n",
              "                            <tr>\r\n",
              "                                <td style=\"text-align: left; width: 25ch\">[1, 0]</td>\r\n",
              "                                <td style=\"text-align: left; width: 25ch\">0.7</td>\r\n",
              "                                <td style=\"text-align: left; width: calc(100% - 25ch - 25ch)\">\r\n",
              "                                    <progress\r\n",
              "                                        max=\"100\"\r\n",
              "                                        value=\"70\"\r\n",
              "                                        style=\"width: 100%;\"\r\n",
              "                                    >\r\n",
              "                                </td>\r\n",
              "                            </tr>\r\n",
              "                        \n",
              "\r\n",
              "                            <tr>\r\n",
              "                                <td style=\"text-align: left; width: 25ch\">[0, 1]</td>\r\n",
              "                                <td style=\"text-align: left; width: 25ch\">0.3</td>\r\n",
              "                                <td style=\"text-align: left; width: calc(100% - 25ch - 25ch)\">\r\n",
              "                                    <progress\r\n",
              "                                        max=\"100\"\r\n",
              "                                        value=\"30\"\r\n",
              "                                        style=\"width: 100%;\"\r\n",
              "                                    >\r\n",
              "                                </td>\r\n",
              "                            </tr>\r\n",
              "                        \r\n",
              "                        </tbody>\r\n",
              "                    </table>\r\n",
              "                "
            ],
            "text/plain": [
              "{'[1, 0]': 0.7, '[0, 1]': 0.3}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
